Steps to ingest 1000 Genomes phase 1 data.


===========================================
- Pull table schemas for BigQuery tables into local JSON files. Use JQ to reformat them into the expected JSON format for Data Repo's create dataset command.

cd ~/Workspaces/jade-search-poc/1000Genomes 

bq show --format=prettyjson bigquery-public-data:human_genome_variants.1000_genomes_sample_info | jq '.schema.fields[] |= del(.description)' | jq '.schema.fields[] |= with_entries(if .key == "type" then .key = "datatype" else . end)' | jq '.schema.fields' > ./table-schema/1000_genomes_sample_info.json

bq show --format=prettyjson bigquery-public-data:human_genome_variants.1000_genomes_pedigree | jq '.schema.fields[] |= del(.description)' | jq '.schema.fields[] |= with_entries(if .key == "type" then .key = "datatype" else . end)' | jq '.schema.fields' > ./table-schema/1000_genomes_pedigree.json


===========================================
- Build a create dataset JSON request with that includes the BigQuery table schemas.

cd ~/Workspaces/jade-search-poc/1000Genomes 

jq --argjson sampleInfoColumns "$(cat ./table-schema/1000_genomes_sample_info.json)" '.schema.tables[0].columns = $sampleInfoColumns' ./dataset-create-template.json > ./dataset-create.json

cp ./dataset-create.json ./dataset-create-tmp.json

jq --argjson pedigreeColumns "$(cat ./table-schema/1000_genomes_pedigree.json)" '.schema.tables[1].columns = $pedigreeColumns' ./dataset-create-tmp.json > ./dataset-create.json

rm ./dataset-create-tmp.json


===========================================
- Create a dataset with the JSON request built above.

alias jc=~/Workspaces/jade-data-repo-cli/build/install/jadecli/bin/jadecli

jc profile create --name 1000GenomesProfile --account 00708C-45D19D-27AAFA

jc dataset create --input-json ./dataset-create.json --profile 1000GenomesProfile

If successful, the output looks like:
class DatasetSummaryModel {
    id: 51f7d24b-60b6-4935-9876-6954b66b938d
    name: 1000GenomesDataset
    description: Public dataset 1000 Genomes.
    defaultProfileId: 3185b759-f9e7-4487-9a35-0e776f669864
    additionalProfileIds: null
    createdDate: 2020-02-04T15:06:31.905583Z
}


===========================================
- Export the BQ tabular data into JSON files in a bucket. This is the sample_info and pedigree tables. Then grant the Data Repo service account read access to the bucket.

gsutil mb -p broad-jade-mm gs://broad-jade-mm-ingestdata/

bq extract --destination_format NEWLINE_DELIMITED_JSON 'bigquery-public-data:human_genome_variants.1000_genomes_sample_info' gs://broad-jade-mm-ingestdata/1000_genomes_sample_info.json

bq extract --destination_format NEWLINE_DELIMITED_JSON 'bigquery-public-data:human_genome_variants.1000_genomes_pedigree' gs://broad-jade-mm-ingestdata/1000_genomes_pedigree.json

gsutil iam ch serviceAccount:jade-k8-sa@broad-jade-dev.iam.gserviceaccount.com:roles/storage.objectViewer gs://broad-jade-mm-ingestdata/


===========================================
- Upload the BQ tabular data that does NOT include file references. This is the sample_info and pedigree tables.

jc dataset table load --table sample_info --input-gspath gs://broad-jade-mm-ingestdata/1000_genomes_sample_info.json --input-format json 1000GenomesDataset

jc dataset table load --table pedigree  --input-gspath gs://broad-jade-mm-ingestdata/1000_genomes_pedigree.json --input-format json 1000GenomesDataset

If successful, the output looks like:
Loaded 3500 rows; 0 bad rows skipped
Loaded 3501 rows; 0 bad rows skipped


===========================================
- Upload the BAM files.

gsutil ls -r gs://genomics-public-data/1000-genomes/bam/** > file-load/filepath-list.txt

cat file-load/filepath-list.txt | sed 's/\(gs:\/\/genomics\-public\-data\/1000\-genomes\/bam\/\(\([^\.]*\)\..*\)\)/jc dataset file load --profile 1000GenomesProfile --input-gspath \1 --target-path \/bam\_files\/\2 --mime-type "application\/octet-stream" --description "\3 BAM file" --format json 1000GenomesDataset \&/' > file-load/load-cmd-list.txt

Then run the commands in the file-load/load-cmd-list.txt. I did this in chunks, so the rest of the instructions operate on batches. For me, each file load took about 15-30 minutes to show up in the output of "jc dr tree".

Make a batch file with a subset of the commands. Set the environment variables to the values for the current subest. The example values below pull 6 load commands, line numbers 5 through 11 inclusive.

export LAST_LINE_NUM=5
export NUM_FILES=5
head -$LAST_LINE_NUM file-load/load-cmd-list.txt | tail -$NUM_FILES > file-load/load-cmd-list-batch.txt

Note about batch size: There are 4 stairway threads in the pool, so that is the maximum number of files that can be loaded in parallel. If you make more than 4 requests, they get queued up. The file ingest will make Data Repo unresponsive to other requests if all the threads are occupied.

===========================================
- Fetch the file IDs of the BAM files and build rows for the bam_file table. Then load the table rows.

Make a batch file with a subset of the files. Set the environment variables to the values for the current subest. The values of these environment variables should match the values from the file uploading batch. The bash script also uses the environment variables, so be sure to export them.

head -$LAST_LINE_NUM file-load/filepath-list.txt | tail -$NUM_FILES > file-load/filepath-list-batch.txt

sh ./write-bam-load-json.sh

gsutil cp file-load/bam-table-load.json gs://broad-jade-mm-ingestdata/bam-table-load.json

jc dataset table load --table bam_file --input-gspath gs://broad-jade-mm-ingestdata/bam-table-load.json --input-format json 1000GenomesDataset

If successful, the output looks like:
Loaded 5 rows; 0 bad rows skipped


===========================================
(POST-INGEST)
- Create three Snapshots: 1000Genomes-EntireDataset, 1000Genomes-First2RowsDataset, 1000Genomes-Next3RowsDataset.
- Add my user to the policy on all three Snapshots. Create a second dummy user and only add them to the policy for the First2RowsDataset. Create a third dummy user and only add them to the policy for the Next3RowsDataset.


===========================================
(POSTPONED FOR LATER, SINCE THIS WILL NOT AFFECT THE DATA EXPLORER PROOF-OF-CONCEPT)
- Upload the VCF files.
- Fetch the file IDs of the VCF files and build rows for the vcf_file table. Then load the table rows.

- This won't affect the Data Explorer POC because the VCF files are multi-sample. They are organized by chromosome and contain information from all the samples. So they don't fit the pattern of searching for a subset of samples to build a cohort.

